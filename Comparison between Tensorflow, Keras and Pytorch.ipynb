{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,GlobalAveragePooling2D,Input\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data pipeline of building a model\n",
    "\n",
    "Model:\n",
    "\n",
    "    Forward propagation:\n",
    "        Layer\n",
    "        activation function\n",
    "    Loss function\n",
    "    Optimizer:\n",
    "        Batch gradient desent\n",
    "    Metric\n",
    "    Back propagation\n",
    "    Evaluation\n",
    "    Prediction\n",
    "Save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 8, 8, 1)\n",
      "(1792,)\n",
      "(5, 8, 8, 1)\n",
      "(5,)\n",
      "(1792, 1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "'''For Tensorflow, channel last for input'''\n",
    "features_train=digits.images[:-5][:,:,:,np.newaxis]\n",
    "target_train=digits.target[:-5]\n",
    "\n",
    "features_test=digits.images[-5:][:,:,:,np.newaxis]\n",
    "target_test=digits.target[-5:]\n",
    "\n",
    "'''For Pytorch, channel first for input'''\n",
    "features_torch_train=features_train.reshape(features_train.shape[0],-1,8,8)\n",
    "features_torch_test=features_test.reshape(features_test.shape[0],-1,8,8)\n",
    "\n",
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)\n",
    "print(features_torch_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras API\n",
    "'''\n",
    "tf.keras.preprocessing.image_dataset_from_directory\n",
    "tf.keras.preprocessing.text_dataset_from_directory\n",
    "'''\n",
    "#Tensorflow\n",
    "tf_dataset_train = tf.data.Dataset.from_tensor_slices((features_train, target_train))\n",
    "tf_dataset_test = tf.data.Dataset.from_tensor_slices((features_test, target_test))\n",
    "#Pytorch (Pandas -> Numpy -> Tensor)\n",
    "'''target: dtype=torch.long for nn.CrossEntropyLoss()'''\n",
    "torch_dataset_train = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(features_torch_train,device=device,dtype=torch.float32),\n",
    "    torch.tensor(target_train,device=device,dtype=torch.long))\n",
    "\n",
    "torch_dataset_test = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(features_torch_test,device=device,dtype=torch.float32),\n",
    "    torch.tensor(target_test,device=device,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visiualizing data in tensorflow and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow\n",
      "feature shape: (8, 8, 1),target shape: ()\n",
      "\n",
      "Pytorch\n",
      "feature shape: torch.Size([1, 8, 8]),target shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow')\n",
    "print(f\"feature shape: {next(iter(tf_dataset_train))[0].shape},target shape: {next(iter(tf_dataset_train))[1].shape}\")\n",
    "print()\n",
    "print('Pytorch')\n",
    "print(f\"feature shape: {next(iter(torch_dataset_train))[0].shape},target shape: {next(iter(torch_dataset_train))[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow\n",
    "tf_loader_train=tf_dataset_train.shuffle(len(tf_dataset_train)).batch(32)\n",
    "'''No shuffle for test data'''\n",
    "tf_loader_test=tf_dataset_test.batch(32)\n",
    "#Pytorch\n",
    "torch_loader_train=torch.utils.data.DataLoader(torch_dataset_train,batch_size=32,shuffle=True)\n",
    "'''No shuffle for test data'''\n",
    "torch_loader_test=torch.utils.data.DataLoader(torch_dataset_test,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow\n",
      "feature shape: (32, 8, 8, 1), target shape: (32,)\n",
      "\n",
      "feature shape: torch.Size([32, 1, 8, 8]), target shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow\n",
    "print('Tensorflow')\n",
    "print(f\"feature shape: {next(iter(tf_loader_train))[0].shape}, target shape: {next(iter(tf_loader_train))[1].shape}\")\n",
    "print()\n",
    "print(f\"feature shape: {next(iter(torch_loader_train))[0].shape}, target shape: {next(iter(torch_loader_train))[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model:\n",
    "    Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Keras_API\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 8, 8, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 6, 6, 32)          320       \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "MaxPool (MaxPooling2D)       (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "GlobalPool (GlobalAveragePoo (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Connected (Dense)            (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 9,898\n",
      "Trainable params: 9,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 2.0397 - sparse_categorical_accuracy: 0.3990\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.9048 - sparse_categorical_accuracy: 0.8080\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4436 - sparse_categorical_accuracy: 0.9023\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2749 - sparse_categorical_accuracy: 0.9364\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2220 - sparse_categorical_accuracy: 0.9492\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1717 - sparse_categorical_accuracy: 0.9621\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1363 - sparse_categorical_accuracy: 0.9671\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.9738\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1009 - sparse_categorical_accuracy: 0.9777\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0938 - sparse_categorical_accuracy: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d2890e18c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras API\n",
    "input_shape = (8,8,1)\n",
    "classes=10\n",
    "\n",
    "def Keras_API(input_shape,classes):\n",
    "    X_input = keras.layers.Input(input_shape,name='Input')\n",
    "    X = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\",name=\"Conv1\")(X_input)\n",
    "    X = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\",name=\"Conv2\")(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2),name=\"MaxPool\")(X)\n",
    "    X = GlobalAveragePooling2D(name=\"GlobalPool\")(X)\n",
    "    outputs = Dense(classes, activation=\"softmax\",name=\"Connected\")(X)\n",
    "    model = keras.models.Model(inputs = X_input, outputs = outputs, name='Keras_API')\n",
    "    return model\n",
    "\n",
    "model=Keras_API(input_shape,classes)\n",
    "print(model.summary())\n",
    "\n",
    "'''Initiate \n",
    "optimizer, loss and metric in model.compile\n",
    "batch_size, epochs in model.fit'''\n",
    "loss_fn=keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer='Adam',loss=loss_fn,metrics='sparse_categorical_accuracy')\n",
    "model.fit(features_train, target_train,batch_size=32, epochs=10,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.0880 - sparse_categorical_accuracy: 1.0000\n",
      "[9 0 8 9 8]\n",
      "[9 0 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(features_test,target_test)\n",
    "\n",
    "pred_test=model.predict(features_test)\n",
    "'''\n",
    "Pred_test is an array with the probability of 10 classes\n",
    "Use np.argmax to extract the index of highest porbability\n",
    "'''\n",
    "print(np.argmax(pred_test,axis=1))\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow\n",
    "'''\n",
    "You can self defined layer and model via\n",
    "tf.keras.layers.Layer\n",
    "tf.keras.Model\n",
    "'''\n",
    "'''\n",
    "class self_define_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,variable):\n",
    "        super(self,self_define_layer).__init__()\n",
    "    def call(self,):\n",
    "        return\n",
    "'''\n",
    "class TF_model(keras.Model):\n",
    "    def __init__(self,filters=32,kernel=(3,3),pool_size=(2,2),classes=10):\n",
    "        super(TF_model,self).__init__()\n",
    "        self.filters=filters\n",
    "        self.kernel=kernel\n",
    "        self.pool_size=pool_size\n",
    "        self.classes=classes\n",
    "        \n",
    "        self.conv2d_1=Conv2D(filters=self.filters, kernel_size=self.kernel, activation=\"relu\")\n",
    "        self.conv2d_2=Conv2D(filters=self.filters, kernel_size=self.kernel, activation=\"relu\")\n",
    "        self.maxpool=MaxPooling2D(pool_size=self.pool_size)\n",
    "        self.globalpool=GlobalAveragePooling2D()\n",
    "        self.dense=Dense(self.classes, activation=\"softmax\")\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x=self.conv2d_1(inputs)\n",
    "        x=self.conv2d_2(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=self.globalpool(x)\n",
    "        output=self.dense(x)\n",
    "        return output\n",
    "TF=TF_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer tf_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch:1, loss: 1.0790282487869263, Accuracy: 0.5128348469734192\n",
      "Epoch:2, loss: 0.39115095138549805, Accuracy: 0.875\n",
      "Epoch:3, loss: 0.3408812880516052, Accuracy: 0.9285714030265808\n",
      "Epoch:4, loss: 0.14485768973827362, Accuracy: 0.9425223469734192\n",
      "Epoch:5, loss: 0.17826536297798157, Accuracy: 0.9620535969734192\n",
      "Epoch:6, loss: 0.10369610786437988, Accuracy: 0.9659598469734192\n",
      "Epoch:7, loss: 0.11171208322048187, Accuracy: 0.9659598469734192\n",
      "Epoch:8, loss: 0.14070822298526764, Accuracy: 0.9743303656578064\n",
      "Epoch:9, loss: 0.15226443111896515, Accuracy: 0.9827008843421936\n",
      "Epoch:10, loss: 0.12966494262218475, Accuracy: 0.9782366156578064\n"
     ]
    }
   ],
   "source": [
    "'''Clean model parameters for previous training'''\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "epochs=10\n",
    "'''Define Optimizer and Metric'''\n",
    "loss_fn=keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc_metric.reset_states()\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(tf_loader_train):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = TF(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, pred)\n",
    "\n",
    "        gradients = tape.gradient(loss_value, TF.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, TF.trainable_weights))\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train,pred)\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"Epoch:{epoch+1}, loss: {loss_value.numpy()}, Accuracy: {train_acc.numpy()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9783\n",
      "[9 0 8 9 8]\n",
      "tf.Tensor([9 0 8 9 8], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "TF.compile(loss=loss_fn, metrics=train_acc_metric)\n",
    "TF.evaluate(tf_loader_test)\n",
    "pred_test=TF.predict(tf_loader_test)\n",
    "print(np.argmax(pred_test,axis=1))\n",
    "print(list(iter(tf_loader_test))[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 32, 6, 6]             320\n",
      "            Conv2d-2             [-1, 32, 4, 4]           9,248\n",
      "         MaxPool2d-3             [-1, 32, 2, 2]               0\n",
      "         AvgPool2d-4             [-1, 32, 1, 1]               0\n",
      "            Linear-5                   [-1, 10]             330\n",
      "================================================================\n",
      "Total params: 9,898\n",
      "Trainable params: 9,898\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Pytorch\n",
    "input_shape=(1,8,8)\n",
    "class Pytorch_model(nn.Module):\n",
    "    def __init__(self,filters=32,kernel=(3,3),pool_size=(2,2),classes=10):\n",
    "        super(Pytorch_model, self).__init__()\n",
    "        self.filters=filters\n",
    "        self.kernel=kernel\n",
    "        self.pool_size=pool_size\n",
    "        self.classes=classes\n",
    "        ''' For Conv2d: (input channel, output channel(number of filters), kernel size)'''\n",
    "        self.Conv1=nn.Conv2d(1, filters ,kernel_size = kernel)\n",
    "        self.Conv2=nn.Conv2d(32, filters ,kernel_size = kernel)\n",
    "        self.MaxPool=nn.MaxPool2d(self.pool_size)\n",
    "        '''Average Pooling:Dimension of the last feature map'''\n",
    "        self.AvgPool=nn.AvgPool2d(2)\n",
    "        self.Dense=nn.Linear(filters*1*1,classes)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        X = F.relu(self.Conv1(inputs))\n",
    "        X = F.relu(self.Conv2(X))\n",
    "        X = self.MaxPool(X)\n",
    "        X = self.AvgPool(X)\n",
    "        '''Flatten tensor'''\n",
    "        X = X.view(-1, 32 * 1 * 1)\n",
    "        X = self.Dense(X)\n",
    "        return X\n",
    "    \n",
    "pytorch_model=Pytorch_model().cuda()\n",
    "print(summary(pytorch_model, input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, loss: 2.192021369934082\n",
      "Epoch:2, loss: 2.0370450019836426\n",
      "Epoch:3, loss: 1.9012563228607178\n",
      "Epoch:4, loss: 1.6298552751541138\n",
      "Epoch:5, loss: 1.3796319961547852\n",
      "Epoch:6, loss: 1.3091481924057007\n",
      "Epoch:7, loss: 0.9962636828422546\n",
      "Epoch:8, loss: 0.8420236110687256\n",
      "Epoch:9, loss: 0.6224917769432068\n",
      "Epoch:10, loss: 0.6451207995414734\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "'''Seems no metric for pytorch, but found metric function in pytorch lightning'''\n",
    "\n",
    "'''for CrossEntropyLoss(), the shape of target should be 1-D tensor, ie,torch.Size([batch size]'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.0001,betas=(0.9, 0.999))\n",
    "\n",
    "epochs=10\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(torch_loader_train):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = pytorch_model(x_batch_train)\n",
    "        loss = criterion(outputs, y_batch_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch:{epoch+1}, loss: {loss}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 8, 9, 8]\n",
      "[9, 0, 8, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "pred_pytorch=[]\n",
    "targets=[]\n",
    "with torch.no_grad():\n",
    "        for data,target in torch_loader_test:\n",
    "            pred=pytorch_model(data)\n",
    "            pred_pytorch.append(torch.argmax(pred).item())\n",
    "            targets.append(target.item())\n",
    "print(pred_pytorch)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
